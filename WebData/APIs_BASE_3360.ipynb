{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using APIs to Gather Data\n",
    "### by [Jason DeBacker](http://jasondebacker.com), October 2017\n",
    "\n",
    "This notebook provides a tutorial and examples showing how to use APIs and Python in order to gather data relevant for economic research.\n",
    "\n",
    "## What is an API?\n",
    "\n",
    "API stands for \"Application Programming Interface\".  APIs provide a way for one piece of software to interact with another by sharing data.  As such, APIs are very general and have many uses.  For example, APIs are used when you copy something from your internet browser and paste it into your word processor.  Here, we are going to focus on a relatively narrow application of APIs.  Specifically, we'll focus on using APIs to help use compile data that we can use for research.\n",
    "\n",
    "## Why use an API?\n",
    "\n",
    "A relevant question is why one would want to use an API rather than downloading a static datafile.  There are a few reasons for using APIs over static data:\n",
    "1. Static datasets may not be available for the particular data source you are interesting in.\n",
    "2. Static datasets may be too large to want to deal with - and your research involves just a selection of these data.\n",
    "3. The data is changing quickly.\n",
    "\n",
    "The last may be less relevant to academic research given our timelines, but the first two are certainly applicable.\n",
    "\n",
    "## The JSON file format\n",
    "\n",
    "Before we get started using APIs, lets discuss the [JSON (JavaScript Object Notation)](http://www.json.org) file format. JSON has become a standard data foramt and many APIs pass data as JSON files.  The JSON format is human-readable, but also easy for machines to parse and generate.  JSON is based on key-value pairs (like a Python dictionary) and ordered lists.  Here's an example of what JSON data look like:\n",
    "\n",
    "```\n",
    "{\n",
    "     \"firstName\": \"John\",\n",
    "     \"lastName\": \"Smith\",\n",
    "     \"address\": {\n",
    "         \"streetAddress\": \"21 2nd Street\",\n",
    "         \"city\": \"New York\",\n",
    "         \"state\": \"NY\",\n",
    "         \"postalCode\": 10021\n",
    "     },\n",
    "     \"phoneNumbers\": [\n",
    "         \"212 555-1234\",\n",
    "         \"646 555-4567\"\n",
    "     ]\n",
    " }\n",
    "```\n",
    "\n",
    "You can see some similarities between the what JSON is written and the way one writes data to a Python dictioinary.  They contain similar structures.  Another thing to notice about the JSON format - you can clearly understand the data that are being represented here and what they mean.\n",
    "\n",
    "## Example 1: Using `pandas-datareader`\n",
    "\n",
    "One of the easiest ways to use an API is Python is to use the `pandas-datareader` package.  Fortunately for our purposes, this package also provides us the ability to connect to APIs for a number of datasources that are commonly used in economics such as:\n",
    "\n",
    "* Quandl\n",
    "* St.Louis FED (FRED)\n",
    "* Kenneth Frenchâ€™s data library\n",
    "* World Bank\n",
    "* OECD\n",
    "* Yahoo! Finance\n",
    "* Google Finance\n",
    "* Eurostat\n",
    "* Enigma\n",
    "\n",
    "For the example that follows, we will gather some data from FRED and plot it.  `pandas-datareader` provides a wrapper for the [FRED API](https://research.stlouisfed.org/docs/api/fred/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas-datareader and other packages\n",
    "import pandas_datareader.data as web\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# set beginning and end dates for data\n",
    "start = datetime.datetime(1947, 1, 1) # format is year (1947), month (1), day (1)\n",
    "end = datetime.date.today() # go through today\n",
    "\n",
    "# pull series of interest using pandas_datareader\n",
    "fred_data=web.DataReader([\"USREC\", \"GDPC1\", \"GPDIC96\"], \"fred\", start, end)\n",
    "fred_data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some data are monthly - put everything into quartely by averaging over months\n",
    "fred_data = fred_data.resample('Q').mean()\n",
    "fred_data = fred_data[:-1] # drop last quarter since not all data available\n",
    "\n",
    "# First-difference and detrend the data\n",
    "# Note: probably a better way to do this\n",
    "fred_data['dt_gdp']=fred_data['GDPC1'].pct_change(freq='Q') - np.mean(fred_data['GDPC1'].pct_change(freq='Q'))\n",
    "fred_data['dt_inv']=fred_data['GPDIC96'].pct_change(freq='Q') - np.mean(fred_data['GPDIC96'].pct_change(freq='Q'))\n",
    "\n",
    "# plot GDP and investment series\n",
    "ax = fred_data.plot(y=['dt_gdp', 'dt_inv'], title=\"Volatility of GDP and Investment\",\n",
    "               grid=True, lw=1, color = ['blue', 'red'])\n",
    "# Label y-axis\n",
    "ax.set_ylabel(\"Percent Change from Previous Quarter\")\n",
    "# rename column vars for legend\n",
    "ax.legend(['GDP', 'Investment'])\n",
    "# add a zero line\n",
    "ax.axhline(0, color='k', linestyle='-', linewidth=1)\n",
    "# shade recessions\n",
    "ax.fill_between(fred_data.index, ax.get_ylim()[0], ax.get_ylim()[1], where=fred_data['USREC'].values, color='gray', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty neat.  We can, with few line of code, gather and plot any data that FRED has.  `pandas-datareader` is doing a lot behind the scenes.  It is contacting the web address of the FRED servers which store these data, making a request to get these data, downloading those JSON files, and then creating the `fred_data` dataframe object from those JSON files. \n",
    "\n",
    "## Example 2:  Accessing Trade Data\n",
    "\n",
    "Now we'll use an API for which there is not a Python wrapper.  In particular, we'll download trade data from the Observatory of Economic Complexity (OEC).  The OEC has compiled trade data from sources such as the United Nations Statistical Division (COMTRADE) and put them in one place.  These databases are large - trade flows by very dissaggregated levels of goods between all most country pairs from 1962-2015.\n",
    "\n",
    "Fortunately, the OEC has an API we can use to access these data.  Details on how to use the API are [here](http://atlas.media.mit.edu/api/).  Let's use this and look at some trends in trade flows between the U.S. and China."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Format for URL: / CLASSIFICATION / TRADE_FLOW / YEAR / ORIGIN / DESTINATION / PRODUCT /\n",
    "url = \"http://atlas.media.mit.edu/sitc/export/1962.2015/chn/usa/all/\"\n",
    "response = requests.get(url)\n",
    "trade_data = response.text\n",
    "trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(trade_data, orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.style.use('seaborn')\n",
    "# use data just for one SITC category - Rubber tires, SITC code 106251\n",
    "df[df['sitc_id']==106251].plot(x='year', y=['import_val','export_val'], kind='bar',\n",
    "                              title='Chinese imports and exports by year - Rubber tires')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Twitter data\n",
    "\n",
    "### Setting up\n",
    "\n",
    "To use the Twitter API, you will need to have some credentials to authenticate yourself.  You can set these up by going to [https://apps.twitter.com](https://apps.twitter.com) and creating a new app.  This will allow you to get the credentials you need: `Consumer Key` and `Consumer Secret`. Depending on what you are doing, you might also need the `Access Token Key` and the `Access Token Secret`.  Grab all of these and store them in a file THAT YOU DO NOT SHARE (these are private).  I put these in a JSON file called `credentials.json`.  It contains the follwing (with my credentials removed):\n",
    "\n",
    "```\n",
    "{\n",
    "  \"consumer_key\" : \"...\",\n",
    "  \"consumer_secret\" : \"...\",\n",
    "  \"access_token_key\" : \"...\",\n",
    "  \"access_token_secret\" : \"...\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The Twitter API has some notable limitations.  For example, the twitter API limits your search results to 100 tweets (in many circumstances, some times you can get more, but still not many thousands).  You can get more tweets by opening up a streaming API to gather tweets in realtime.  This can be done with they `TwythonStreaming` module - or with other packages that interface with the Twitter API.  \n",
    "\n",
    "Note there are also Twitter APIs for enterprise use that allow you to gather more data, but they cost $$.\n",
    "\n",
    "### Searching for tweets with a specific query\n",
    "\n",
    "First, let's see how to grab tweets with a query.  E.g., we can try to grab tweets with a certain phrase in them.  There are seveal Python packages that provide wrappers or the Twitter API.  We'll use [Twython](https://twython.readthedocs.io/en/latest/).  [Tweepy](https://github.com/tweepy/tweepy) seems to also be popular.  But they all have similar structure, so once you get the handle on one, you can probably use others without much of a learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from twython import Twython\n",
    "from twython.exceptions import TwythonError\n",
    "import json\n",
    "\n",
    "# import credentials to connect to Twitter API\n",
    "with open('credentials.json') as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "# Initiate the client\n",
    "client = Twython(\n",
    "    creds['consumer_key'],\n",
    "    creds['consumer_secret'],\n",
    "    creds['access_token_key'],\n",
    "    creds['access_token_secret']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the search terms - and filter out retweets\n",
    "query = '#MAGA -filter:retweets'\n",
    "\n",
    "# use the client to run a search\n",
    "# geocode = '25.032341,55.385557,100000mi' # latitude,longitude,distance(mi/km)\n",
    "\n",
    "results = client.search(q=query, count=100)\n",
    "# results = client.search(q=query, count=100, geocode=geocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at how the results are structured\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(results['statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a loot at data from a single tweet\n",
    "results['statuses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put results in a DataFrame\n",
    "df = pd.DataFrame(results['statuses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for tweets from a specific user\n",
    "\n",
    "Another option we have with Twython is to search for results from a specific user.  Here we need to provide a screen name or user id.  We can return up to about the 200 most recent tweets from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "tweet_loc = []\n",
    "try:\n",
    "    user_timeline = client.get_user_timeline(screen_name='realdonaldtrump',count=200)\n",
    "except TwythonError as e:\n",
    "    print(e)\n",
    "print(len(user_timeline))\n",
    "for tweet in user_timeline:\n",
    "    # Add whatever you want from the tweet, here we just add the text\n",
    "    tweets.append(tweet['text'])\n",
    "    tweet_loc.append(tweet['geo'])\n",
    "# Count could be less than 200, see:\n",
    "# https://dev.twitter.com/discussions/7513\n",
    "while len(user_timeline) != 0: \n",
    "    try:\n",
    "        user_timeline = client.get_user_timeline(screen_name='realdonaltrump',\n",
    "                                                 count=500,max_id=user_timeline[len(user_timeline)-1]['id']-1)\n",
    "    except TwythonError as e:\n",
    "        print(e)\n",
    "    print(len(user_timeline))\n",
    "    for tweet in user_timeline:\n",
    "        # Add whatever you want from the tweet, here we just add the text\n",
    "        tweets.append(tweet['text'])\n",
    "        tweet_loc.append(tweet['geo'])\n",
    "# Number of tweets the user has made\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking at what is in the tweets we've compiled\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each tweet has much more infor than just the text we saved\n",
    "tweet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the tweets in a dataframe\n",
    "trump_df = pd.DataFrame(tweets)\n",
    "trump_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the frequency of certain text\n",
    "trump_df['Great'] = trump_df[0].str.contains('Great', case=False)\n",
    "trump_df['Trump'] = trump_df[0].str.contains('Trump')\n",
    "trump_df['America'] = trump_df[0].str.contains('America')\n",
    "trump_df['Tax'] = trump_df[0].str.contains('Tax', case=False)\n",
    "trump_df[[\"Great\", \"NFL\", \"Trump\", \"America\", \"Tax\"]].sum().plot(kind = \"bar\", title='Trump Tweet Topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
